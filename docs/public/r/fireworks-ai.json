{
  "name": "fireworks-ai",
  "type": "registry:block",
  "title": "Fireworks AI",
  "description": "Fast inference for open-source AI models",
  "registryDependencies": [],
  "dependencies": ["@vercel/workflow"],
  "files": [
    {
      "path": "registry/steps/fireworks-ai.tsx",
      "type": "registry:component",
      "content": "import { fatalError } from '@vercel/workflow';\n\ntype FireworksParams = {\n  messages: { role: string; content: string }[];\n  model?: string;\n};\n\nexport async function fireworksAI(params: FireworksParams) {\n  'use step';\n\n  const apiKey = process.env.FIREWORKS_API_KEY;\n\n  if (!apiKey) {\n    throw fatalError('FIREWORKS_API_KEY is required');\n  }\n\n  const response = await fetch(\n    'https://api.fireworks.ai/inference/v1/chat/completions',\n    {\n      method: 'POST',\n      headers: {\n        Authorization: `Bearer ${apiKey}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        model:\n          params.model || 'accounts/fireworks/models/llama-v3p1-70b-instruct',\n        messages: params.messages,\n      }),\n    }\n  );\n\n  if (!response.ok) {\n    throw fatalError(`Fireworks API error: ${response.status}`);\n  }\n\n  const data = await response.json();\n  return data.choices[0].message.content;\n}\n",
      "target": "components/steps/fireworks-ai.tsx"
    }
  ]
}
